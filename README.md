# Evolution of Language Models to Large Language Models
* This repo aims to cover the evolution of language models and summarises all the important papers.
* The headers contain the link to the summarization for that particular paper and below is the link for the research paper

## [Genertive Pretrained Transformer (GPT) 2018](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=AP2x1jC9-319&line=1&uniqifier=1)
* [*Paper: Improving Language Understanding by Generative Pre-Training*](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

## [Bidirectional Encoder Representations from Transformers (BERT) 2019](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=EPW2ASRQD01O)
* [*Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/pdf/1810.04805.pdf)

## [GPT-2 2019](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=yHOofcd8Jajj)
* [*Paper: Language Models are Unsupervised Multitask Learners*](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

## [DistilBERT 2020](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=OQiMRitRm9mA)
* [*Paper: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*](https://arxiv.org/pdf/1910.01108.pdf)

## [T5 2020](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=MA83M9eh2pgI)
* [*Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*](https://arxiv.org/pdf/1910.10683.pdf)

## [GPT-3 2020](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=mlHE3Xmjo290)
* [*Paper: Language Models are Few-Shot Learners*](https://arxiv.org/pdf/2005.14165.pdf)

## [PALM-1 2022](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=Vq67GrxcaIS1)
* [*Paper: PaLM: Scaling Language Modeling with Pathways*](https://arxiv.org/pdf/2204.02311.pdf)
  
## [LLaMA-1 and 2 2023](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=Kcpb6FXd1D3B)
* [*LLaMA Paper: LLaMA: Open and Efficient Foundation Language Models*](https://arxiv.org/pdf/2302.13971.pdf)
* [*LLaMA 2 Paper: Llama 2: Open Foundation and Fine-Tuned Chat Models*](https://arxiv.org/pdf/2307.09288.pdf)
* [*RMSNormalization Paper: Root Mean Square Layer Normalization*](https://arxiv.org/pdf/1910.07467v1.pdf)
* [*Rotary Positional Embeddings Paper: ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING*](https://arxiv.org/pdf/2104.09864v5.pdf)
  
## [Mistral 7B 2023](https://colab.research.google.com/drive/1d4BmKVoNGREQR2j2yv9lHORrcWS4eLgl#scrollTo=B8aVv7tF-UWH)
* [*Paper: Mistral 7B*](https://arxiv.org/pdf/2310.06825.pdf)
